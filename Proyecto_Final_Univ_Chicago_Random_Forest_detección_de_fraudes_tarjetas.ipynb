{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walteremoore/ml_deteccion_fraudes_univ_chicago/blob/main/Proyecto_Final_Univ_Chicago_Random_Forest_detecci%C3%B3n_de_fraudes_tarjetas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8jlMKFRAW4C"
      },
      "source": [
        "# Proyecto Final - Univ. Chicago - Inteligencia Artificial y Data Science para Directivos\n",
        "## Algoritmo predictivo de detección temprana de transacciones fraudulentas que cancele las mismas hasta su verificación mediante Machine Learning con posibilidad de escalar. Modelo escogido “Random Forest”.\n",
        "\n",
        "In \\[1\\]:\n",
        "\n",
        "    import pandas as pd\n",
        "    #df=pd.read_csv('creditcard.csv',nrows=100000)\n",
        "    #df.to_csv('creditsample.csv')\n",
        "    df=pd.read_csv('creditsample.csv',index_col=[0])\n",
        "    # URL del dataframe CSV = https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download\n",
        "In \\[2\\]:\n",
        "\n",
        "    df.head(30)\n",
        "\n",
        "Out\\[2\\]:\n",
        "\n",
        "|     | Time | V1        | V2        | V3        | V4        | V5        | V6        | V7        | V8        | V9        | ... | V21       | V22       | V23       | V24       | V25       | V26       | V27       | V28       | Cantidad | Clase |\n",
        "|-----|------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|----------|-------|\n",
        "| 0   | 0    | -1,359807 | -0,072781 | 2,536347  | 1,378155  | -0,338321 | 0,462388  | 0,239599  | 0,098698  | 0,363787  | ... | -0.018307 | 0.277838  | -0.110474 | 0.066928  | 0,128539  | -0,189115 | 0,133558  | -0,021053 | 149,62   | 0     |\n",
        "| 1   | 0    | 1.191857  | 0.266151  | 0,166480  | 0,448154  | 0,060018  | -0,082361 | -0,078803 | 0,085102  | -0,255425 | ... | -0,225775 | -0,638672 | 0,101288  | -0,339846 | 0,167170  | 0,125895  | -0,008983 | 0,014724  | 2,69     | 0     |\n",
        "| 2   | 1    | -1,358354 | -1,340163 | 1,773209  | 0,379780  | -0,503198 | 1,800499  | 0,791461  | 0,247676  | -1,514654 | ... | 0,247998  | 0,771679  | 0,909412  | -0,689281 | -0327642  | -0,139097 | -0,055353 | -0,059752 | 378,66   | 0     |\n",
        "| 3   | 1    | -0,966272 | -0,185226 | 1,792993  | -0,863291 | -0,010309 | 1,247203  | 0,237609  | 0,377436  | -1,387024 | ... | -0,108300 | 0,005274  | -0,190321 | -1,175575 | 0,647376  | -0,221929 | 0,062723  | 0,061458  | 123,50   | 0     |\n",
        "| 4   | 2    | -1,158233 | 0,877737  | 1,548718  | 0,403034  | -0,407193 | 0,095921  | 0,592941  | -0,270533 | 0,817739  | ... | -0,009431 | 0,798278  | -0,137458 | 0,141267  | -0,206010 | 0,502292  | 0,219422  | 0,215153  | 69,99    | 0     |\n",
        "| 5   | 2    | -0,425966 | 0,960523  | 1,141109  | -0,168252 | 0,420987  | -0,029728 | 0,476201  | 0,260314  | -0,568671 | ... | -0,208254 | -0,559825 | -0,026398 | -0,371427 | -0,232794 | 0,105915  | 0,253844  | 0,081080  | 3,67     | 0     |\n",
        "| 6   | 4    | 1,229658  | 0,141004  | 0,045371  | 1,202613  | 0,191881  | 0,272708  | -0,005159 | 0,081213  | 0,464960  | ... | -0,167716 | -0,270710 | -0,154104 | -0,780055 | 0,750137  | -0,257237 | 0,034507  | 0,005168  | 4,99     | 0     |\n",
        "| 7   | 7    | -0,644269 | 1,417964  | 1,074380  | -0,492199 | 0,948934  | 0,428118  | 1,120631  | -3,807864 | 0,615375  | ... | 1,943465  | -1,015455 | 0,057504  | -0,649709 | -0,415267 | -0,051634 | -1,206921 | -1,085339 | 40,80    | 0     |\n",
        "| 8   | 7    | -0,894286 | 0,286157  | -0,113192 | -0,271526 | 2,669599  | 3,721818  | 0,370145  | 0,851084  | -0,392048 | ... | -0,073425 | -0,268092 | -0,204233 | 1,011592  | 0,373205  | -0,384157 | 0,011747  | 0,142404  | 93,20    | 0     |\n",
        "| 9   | 9    | -0,338262 | 1,119593  | 1,044367  | -0,222187 | 0,499361  | -0,246761 | 0,651583  | 0,069539  | -0,736727 | ... | -0,246914 | -0,633753 | -0,120794 | -0,385050 | -0,069733 | 0,094199  | 0,246219  | 0,083076  | 3,68     | 0     |\n",
        "| 10  | 10   | 1,449044  | -1,176339 | 0,913860  | -1,375667 | -1,971383 | -0,629152 | -1,423236 | 0,048456  | -1,720408 | ... | -0,009302 | 0,313894  | 0,027740  | 0,500512  | 0,251367  | -0,129478 | 0,042850  | 0,016253  | 7,80     | 0     |\n",
        "| 11  | 10   | 0,384978  | 0,616109  | -0,874300 | -0,094019 | 2,924584  | 3,317027  | 0,470455  | 0,538247  | -0,558895 | ... | 0,049924  | 0,238422  | 0,009130  | 0,996710  | -0,767315 | -0,492208 | 0,042472  | -0,054337 | 9,99     | 0     |\n",
        "| 12  | 10   | 1,249999  | -1,221637 | 0,383930  | -1,234899 | -1,485419 | -0,753230 | -0,689405 | -0,227487 | -2,094011 | ... | -0,231809 | -0,483285 | 0,084668  | 0,392831  | 0,161135  | -0,354990 | 0,026416  | 0,042422  | 121,50   | 0     |\n",
        "| 13  | 11   | 1,069374  | 0,287722  | 0,828613  | 2,712520  | -0,178398 | 0,337544  | -0,096717 | 0,115982  | -0,221083 | ... | -0,036876 | 0,074412  | -0,071407 | 0,104744  | 0,548265  | 0,104094  | 0,021491  | 0,021293  | 27,50    | 0     |\n",
        "| 14  | 12   | -2,791855 | -0,327771 | 1,641750  | 1,767473  | -0,136588 | 0,807596  | -0,422911 | -1,907107 | 0,755713  | ... | 1,151663  | 0,222182  | 1,020586  | 0,028317  | -0,232746 | -0,235557 | -0,164778 | -0,030154 | 58,80    | 0     |\n",
        "| 15  | 12   | -0,752417 | 0,345485  | 2,057323  | -1,468643 | -1,158394 | -0,077850 | -0,608581 | 0,003603  | -0,436167 | ... | 0,499625  | 1,353650  | -0,256573 | -0,065084 | -0,039124 | -0,087086 | -0,180998 | 0,129394  | 15,99    | 0     |\n",
        "| 16  | 12   | 1,103215  | -0,040296 | 1,267332  | 1,289091  | -0,735997 | 0,288069  | -0,586057 | 0,189380  | 0,782333  | ... | -0,024612 | 0,196002  | 0,013802  | 0,103758  | 0,364298  | -0,382261 | 0,092809  | 0,037051  | 12,99    | 0     |\n",
        "| 17  | 13   | -0,436905 | 0,918966  | 0,924591  | -0,727219 | 0,915679  | -0,127867 | 0,707642  | 0,087962  | -0,665271 | ... | -0,194796 | -0,672638 | -0,156858 | -0,888386 | -0,342413 | -0,049027 | 0,079692  | 0,131024  | 0,89     | 0     |\n",
        "| 18  | 14   | -5,401258 | -5,450148 | 1,186305  | 1,736239  | 3,049106  | -1,763406 | -1,559738 | 0,160842  | 1,233090  | ... | -0,503600 | 0,984460  | 2,458589  | 0,042119  | -0,481631 | -0,621272 | 0,392053  | 0,949594  | 46,80    | 0     |\n",
        "| 19  | 15   | 1,492936  | -1,029346 | 0,454795  | -1,438026 | -1,555434 | -0,720961 | -1,080664 | -0,053127 | -1,978682 | ... | -0,177650 | -0,175074 | 0,040002  | 0,295814  | 0,332931  | -0,220385 | 0,022298  | 0,007602  | 5,00     | 0     |\n",
        "| 20  | 16   | 0,694885  | -1,361819 | 1,029221  | 0,834159  | -1,191209 | 1,309109  | -0,878586 | 0,445290  | -0,446196 | ... | -0,295583 | -0,571955 | -0,050881 | -0,304215 | 0,072001  | -0,422234 | 0,086553  | 0,063499  | 231,71   | 0     |\n",
        "| 21  | 17   | 0,962496  | 0,328461  | -0,171479 | 2,109204  | 1,129566  | 1,696038  | 0,107712  | 0,521502  | -1,191311 | ... | 0,143997  | 0,402492  | -0,048508 | -1,371866 | 0,390814  | 0,199964  | 0,016371  | -0,014605 | 34,09    | 0     |\n",
        "| 22  | 18   | 1,166616  | 0,502120  | -0,067300 | 2,261569  | 0,428804  | 0,089474  | 0,241147  | 0,138082  | -0,989162 | ... | 0,018702  | -0,061972 | -0,103855 | -0,370415 | 0,603200  | 0,108556  | -0,040521 | -0,011418 | 2,28     | 0     |\n",
        "| 23  | 18   | 0,247491  | 0,277666  | 1,185471  | -0,092603 | -1,314394 | -0,150116 | -0,946365 | -1,617935 | 1,544071  | ... | 1,650180  | 0,200454  | -0,185353 | 0,423073  | 0,820591  | -0,227632 | 0,336634  | 0,250475  | 22,75    | 0     |\n",
        "| 24  | 22   | -1,946525 | -0,044901 | -0,405570 | -1,013057 | 2,941968  | 2,955053  | -0,063063 | 0,855546  | 0,049967  | ... | -0,579526 | -0,799229 | 0,870300  | 0,983421  | 0,321201  | 0,149650  | 0,707519  | 0,014600  | 0,89     | 0     |\n",
        "| 25  | 22   | -2,074295 | -0,121482 | 1,322021  | 0,410008  | 0,295198  | -0,959537 | 0,543985  | -0,104627 | 0,475664  | ... | -0,403639 | -0,227404 | 0,742435  | 0,398535  | 0,249212  | 0,274404  | 0,359969  | 0,243232  | 26,43    | 0     |\n",
        "| 26  | 23   | 1,173285  | 0,353498  | 0,283905  | 1,133563  | -0,172577 | -0,916054 | 0,369025  | -0,327260 | -0,246651 | ... | 0,067003  | 0,227812  | -0,150487 | 0,435045  | 0,724825  | -0,337082 | 0,016368  | 0,030041  | 41,88    | 0     |\n",
        "| 27  | 23   | 1,322707  | -0,174041 | 0,434555  | 0,576038  | -0,836758 | -0,831083 | -0,264905 | -0,220982 | -1,071425 | ... | -0,284376 | -0,323357 | -0,037710 | 0,347151  | 0,559639  | -0,280158 | 0,042335  | 0,028822  | 16,00    | 0     |\n",
        "| 28  | 23   | -0,414289 | 0,905437  | 1,727453  | 1,473471  | 0,007443  | -0,200331 | 0,740228  | -0,029247 | -0,593392 | ... | 0,077237  | 0,457331  | -0,038500 | 0,642522  | -0,183891 | -0,277464 | 0,182687  | 0,152665  | 33,00    | 0     |\n",
        "| 29  | 23   | 1,059387  | -0,175319 | 1,266130  | 1,186110  | -0,786002 | 0,578435  | -0,767084 | 0,401046  | 0,699500  | ... | 0,013676  | 0,213734  | 0,014462  | 0,002951  | 0,294638  | -0,395070 | 0,081461  | 0,024220  | 12,99    | 0     |\n",
        "\n",
        "30 filas × 31 columnas\n",
        "\n",
        "### La primera pregunta que debe hacerse en la detección de anomalías es: \"¿están etiquetados los datos? Es decir, ¿contamos con alguna observación previa que hayamos detectado como fraude en los datos?\".<a href=\"#First-question-to-ask-in-Anomaly-Detection-is:-%22Do-we-have-labeled-data?-In-other-words,-do-we-have-any-previous-observations-we-detected-as-fraud-in-our-data?\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "-   **En caso afirmativo, ¿qué hay que hacer a continuación?**\n",
        "\n",
        "-   **En caso negativo, ¿qué hay que hacer a continuación?**\n",
        "\n",
        "In \\[4\\]:\n",
        "\n",
        "    df['Class'].describe()\n",
        "\n",
        "Out\\[4\\]:\n",
        "\n",
        "    count    100000,00000\n",
        "    mean          0,00223\n",
        "    std           0,04717\n",
        "    min           0,00000\n",
        "    25 %          0,00000\n",
        "    50 %          0,00000\n",
        "    75 %          0,00000\n",
        "    máx           1,00000\n",
        "    Name: Class, dtype: float64\n",
        "\n",
        "In \\[8\\]:\n",
        "\n",
        "    df['Class'].value_counts()\n",
        "\n",
        "Out\\[8\\]:\n",
        "\n",
        "    0    99777\n",
        "    1      223\n",
        "    Name: Class, dtype: int64\n",
        "\n",
        "In \\[4\\]:\n",
        "\n",
        "    # Explorar las variables disponibles en el marco de datos\n",
        "    print(df.info())\n",
        "\n",
        "    # Contar los casos de fraude y de no fraude y mostrarlos en pantalla\n",
        "    occ = df['Class'].value_counts()\n",
        "    print(occ)\n",
        "\n",
        "    # Mostrar en pantalla la proporción de casos de fraude\n",
        "    print(occ / len(df.index))\n",
        "\n",
        "    <class 'pandas.core.frame.DataFrame'>\n",
        "    Int64Index: 100000 entries, 0 to 99999\n",
        "    Data columns (total 31 columns):\n",
        "    Time      100000 non-null int64\n",
        "    V1        100000 non-null float64\n",
        "    V2        100000 non-null float64\n",
        "    V3        100000 non-null float64\n",
        "    V4        100000 non-null float64\n",
        "    V5        100000 non-null float64\n",
        "    V6        100000 non-null float64\n",
        "    V7        100000 non-null float64\n",
        "    V8        100000 non-null float64\n",
        "    V9        100000 non-null float64\n",
        "    V10       100000 non-null float64\n",
        "    V11       100000 non-null float64\n",
        "    V12       100000 non-null float64\n",
        "    V13       100000 non-null float64\n",
        "    V14       100000 non-null float64\n",
        "    V15       100000 non-null float64\n",
        "    V16       100000 non-null float64\n",
        "    V17       100000 non-null float64\n",
        "    V18       100000 non-null float64\n",
        "    V19       100000 non-null float64\n",
        "    V20       100000 non-null float64\n",
        "    V21       100000 non-null float64\n",
        "    V22       100000 non-null float64\n",
        "    V23       100000 non-null float64\n",
        "    V24       100000 non-null float64\n",
        "    V25       100000 non-null float64\n",
        "    V26       100000 non-null float64\n",
        "    V27       100000 non-null float64\n",
        "    V28       100000 non-null float64\n",
        "    Amount    100000 non-null float64\n",
        "    Class     100000 non-null int64\n",
        "    dtypes: float64(29), int64(2)\n",
        "    memory usage: 24.4 MB\n",
        "    None\n",
        "    0    99777\n",
        "    1      223\n",
        "    Name: Class, dtype: int64\n",
        "    0    0.99777\n",
        "    1    0.00223\n",
        "    Name: Class, dtype: float64\n",
        "\n",
        "In \\[5\\]:\n",
        "\n",
        "    # Definir una función para crear un diagrama de dispersión de los datos y las etiquetas\n",
        "    import matplotlib.pyplot as plt  \n",
        "    import pandas as pd  \n",
        "    %matplotlib inline\n",
        "    import numpy as np \n",
        "    def plot_data(X, y):\n",
        "        plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
        "        plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='y')\n",
        "        plt.legend()\n",
        "        return plt.show()\n",
        "\n",
        "    # Crear X e y a partir de la función definida anteriormente\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    y = df.iloc[:,-1].values\n",
        "    X=df.iloc[:, :-1].values\n",
        "\n",
        "\n",
        "    # Trazar gráficos de los datos ejecutando la función de representar gráficamente datos en X e y\n",
        "    plot_data(X, y)\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "## Sí, tenemos algunas etiquetas. <a href=\"#Yes-we-have-some-labels\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "## ¿Son 223 etiquetas de datos (de un total de 100 000) suficientes para ejecutar un algoritmo de aprendizaje de clasificación? <a href=\"#223-data-labels--out-of-100K--sufficient-enough-to-run-a-classification-learning-algorithm?\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[4\\]:\n",
        "\n",
        "    '''\n",
        "    Aplicamos una técnica de upsampling o interpolación para equilibrar la clase minoritaria en los datos.\n",
        "    Prácticamente duplicamos el tamaño de los datos al generar replicas genéricas.\n",
        "    '''\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    #https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
        "\n",
        "\n",
        "    # Definir el método de resampling o remuestreo\n",
        "    method = SMOTE(kind='regular')\n",
        "\n",
        "    # Crear el conjunto de variables al que se le ha aplicado un resampling\n",
        "    X_resampled, y_resampled = method.fit_sample(X, y)\n",
        "\n",
        "    # Representar gráficamente los datos a lo que se les ha aplicado un resampling\n",
        "    plot_data(X_resampled, y_resampled)\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "### Métodos estadísticos convencionales basados en reglas <a href=\"#Conventional-Rule-Based-Statistical-Methods\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[74\\]:\n",
        "\n",
        "    # Ejecutar un comando groupby en nuestras etiquetas y obtener la media de cada variable\n",
        "    df.groupby('Class').mean()\n",
        "\n",
        "    # Implementar una regla para indicar qué casos se detectan como fraude\n",
        "    df['flag_as_fraud'] = np.where(np.logical_and(df['V1'] < -3, df['V3'] < -5), 1, 0)\n",
        "\n",
        "    # Crear una tabla de referencias cruzadas (crosstab) que incluya los casos de fraude detectados frente a los casos de fraude reales\n",
        "    print(pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))\n",
        "\n",
        "    Fraudes detectados 0    1\n",
        "    Fraudes reales             \n",
        "    0              99374  403\n",
        "    1                134   89\n",
        "\n",
        "In \\[7\\]:\n",
        "\n",
        "    a=df.groupby('Class').mean()\n",
        "    a\n",
        "\n",
        "Out\\[7\\]:\n",
        "\n",
        "|       | Time         | V1        | V2        | V3        | V4        | V5        | V6        | V7        | V8        | V9        | ... | V21       | V22       | V23       | V24       | V25       | V26       | V27       | V28       | Cantidad   | flag_as_fraud |\n",
        "|-------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|------------|---------------|\n",
        "| Clase |              |           |           |           |           |           |           |           |           |           |     |           |           |           |           |           |           |           |           |            |               |\n",
        "| 0     | 94838,202258 | 0,008258  | -0,006271 | 0,012171  | -0,007860 | 0,005453  | 0,002419  | 0,009637  | -0,000987 | 0,004467  | ... | -0,001235 | -0,000024 | 0,000070  | 0,000182  | -0,000072 | -0,000089 | -0,000295 | -0,000131 | 88,291022  | 0,004312      |\n",
        "| 1     | 80746,806911 | -4,771948 | 3,623778  | -7,033281 | 4,542029  | -3,151225 | -1,397737 | -5,568731 | 0,570636  | -2,581123 | ... | 0,713588  | 0,014049  | -0,040308 | -0,105130 | 0,041449  | 0,051648  | 0,170575  | 0,075667  | 122,211321 | 0,345528      |\n",
        "\n",
        "2 filas × 31 columnas\n",
        "\n",
        "### *Machine learning* en datos no equilibrados <a href=\"#Machine-Learning-on-Imbalanced-Data\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[6\\]:\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "    y = df.iloc[:,-1].values\n",
        "    X=df.iloc[:, :-1].values\n",
        "    # Crear los conjuntos de entrenamiento y de prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "    # Ajustar un modelo de regresión logística a los datos.\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener predicciones de los modelos\n",
        "    predicted = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)\n",
        "    # Mostrar en pantalla el informe de clasificación y la matriz de confusión\n",
        "    #print('Classification report:\\n', metrics.classification_report(y_test, predicted))\n",
        "    conf_mat = metrics.confusion_matrix(y_true=y_test, y_pred=predicted)\n",
        "    print(roc_auc_score(y_test, probs[:,1]))\n",
        "    print(classification_report(y_test, predicted))\n",
        "    print('Confusion matrix:\\n', conf_mat)\n",
        "\n",
        "    /Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
        "      FutureWarning)\n",
        "\n",
        "    0,9725126081293209\n",
        "                  precisión  exhaust.  f1-score   soporte\n",
        "\n",
        "               0       1,00      1,00      1,00     29941\n",
        "               1       0,76      0,64      0,70        59\n",
        "\n",
        "       prom. micro     1,00      1,00      1,00     30000\n",
        "       prom. macro     0,88      0,82      0,85     30000\n",
        "    prom. ponderado    1,00      1,00      1,00     30000\n",
        "\n",
        "    Matriz de confusión:\n",
        "     [[29929    12]\n",
        "     [   21    38]]\n",
        "\n",
        "In \\[26\\]:\n",
        "\n",
        "    import numpy as np\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
        "\n",
        "Out\\[26\\]:\n",
        "\n",
        "    <matplotlib.axes._subplots.AxesSubplot at 0x11d22aa50>\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "### *Machine learning* con equilibrio mejorado<a href=\"#Machine-Learning-with-improved-balance\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[36\\]:\n",
        "\n",
        "    # Este es el módulo pipeline que necesitamos para imblearn\n",
        "    from imblearn.pipeline import Pipeline \n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    #https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#smote-adasyn\n",
        "    # Definir qué método de resampling y qué modelo de ML se va a utilizar en el pipeline\n",
        "    resampling = SMOTE(kind='borderline2')\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Definir el pipeline, decirle que combine SMOTE con el modelo de regresión logística\n",
        "    pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])\n",
        "\n",
        "In \\[37\\]:\n",
        "\n",
        "    # Dividir los datos X e y en un conjunto de entrenamiento y en un conjunto de prueba y ajustar el pipeline a los datos de entrenamiento\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "    # Ajustar el pipeline al conjunto de entrenamiento y obtener predicciones ajustando el modelo a los datos de prueba \n",
        "    pipeline.fit(X_train, y_train) \n",
        "\n",
        "    /Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
        "      FutureWarning)\n",
        "\n",
        "Out\\[37\\]:\n",
        "\n",
        "    Pipeline(memory=None,\n",
        "         steps=[('SMOTE', SMOTE(k_neighbors=5, kind='borderline-2', m_neighbors=10, n_jobs=1,\n",
        "       out_step='deprecated', random_state=None, ratio=None,\n",
        "       sampling_strategy='auto', svm_estimator='deprecated')), ('Logistic Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "              intercept_scaling=1, max_iter=100, multi_class='warn',\n",
        "              n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
        "              tol=0.0001, verbose=0, warm_start=False))])\n",
        "\n",
        "In \\[38\\]:\n",
        "\n",
        "    predicted = model.predict(X_test)\n",
        "\n",
        "In \\[40\\]:\n",
        "\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
        "\n",
        "\n",
        "    '''\n",
        "    Resultados de la regresión logística para los datos a los que se les ha aplicado resampling (aprox. 200 000 muestras en lugar de 100 000)\n",
        "\n",
        "    '''\n",
        "\n",
        "Out\\[40\\]:\n",
        "\n",
        "    <matplotlib.axes._subplots.AxesSubplot at 0x13faab450>\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "In \\[41\\]:\n",
        "\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted = model.predict(X_test)\n",
        "    y_true=y_test\n",
        "    y_pred=predicted\n",
        "    labels = [0,1]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels)\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
        "\n",
        "    '''\n",
        "    Resultados de random forest para los datos a los que se les ha aplicado resampling (aprox. 200 000 muestras en lugar de 100 000)\n",
        "\n",
        "    '''\n",
        "\n",
        "    /Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
        "      \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
        "\n",
        "Out\\[41\\]:\n",
        "\n",
        "    <matplotlib.axes._subplots.AxesSubplot at 0x140f60250>\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "In \\[42\\]:\n",
        "\n",
        "    from sklearn.svm import SVC\n",
        "    model = SVC(C=0.5, kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted = model.predict(X_test)\n",
        "    y_true=y_test\n",
        "    y_pred=predicted\n",
        "    labels = [0,1]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels)\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
        "\n",
        "    '''\n",
        "    Resultados de la máquina de soporte de vectores para datos ha los que se les ha aplicado resampling (aprox. 200 000 muestras en lugar de 100 000)\n",
        "    '''\n",
        "\n",
        "Out\\[42\\]:\n",
        "\n",
        "    <matplotlib.axes._subplots.AxesSubplot at 0x141126dd0>\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "In \\[43\\]:\n",
        "\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    # Crear una instancia de DecisionTreeClassifier 'dt' con una profundidad máxima de 6\n",
        "    model = DecisionTreeClassifier(max_depth=6, random_state=0)\n",
        "    from sklearn.svm import SVC\n",
        "    model = SVC(C=0.5, kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted = model.predict(X_test)\n",
        "    y_true=y_test\n",
        "    y_pred=predicted\n",
        "    labels = [0,1]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels)\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
        "\n",
        "    '''\n",
        "    Resultados del clasificador de árbol de decisión para datos a los que se les ha aplicado resampling (aprox. 200 000 muestras en lugar de 100 000)\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "Out\\[43\\]:\n",
        "\n",
        "    <matplotlib.axes._subplots.AxesSubplot at 0x11d2bf710>\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "### Calcular la precisión y la exhaustividad para cada modelo. <a href=\"#Calculate-Precision-and-Recall-for-each-model\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Comparar las matrices de confusión para cada modelo y determinar cuál se adaptará mejor a este problema empresarial.<a href=\"#Compare-the-Confusion-Matrices-for-each-model-and-determine-which-model-will-be-a-better-fit-for-this-business-problem\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Límites de decisión lineales y no lineales <a href=\"#Linear-&amp;-Non-linear-Decision-Boundaries\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[5\\]:\n",
        "\n",
        "    import base64, io, IPython\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    image = PILImage.open(\"monns.png\")\n",
        "\n",
        "    output = io.BytesIO()\n",
        "    image.save(output, format='PNG')\n",
        "    encoded_string = base64.b64encode(output.getvalue()).decode()\n",
        "\n",
        "    html = '<img src=\"data:monns.png;base64,{}\"/>'.format(encoded_string)\n",
        "    IPython.display.HTML(html)\n",
        "\n",
        "Out\\[5\\]:\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "In \\[6\\]:\n",
        "\n",
        "    import base64, io, IPython\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    image = PILImage.open(\"irs.png\")\n",
        "\n",
        "    output = io.BytesIO()\n",
        "    image.save(output, format='PNG')\n",
        "    encoded_string = base64.b64encode(output.getvalue()).decode()\n",
        "\n",
        "    html = '<img src=\"data:irs.png;base64,{}\"/>'.format(encoded_string)\n",
        "    IPython.display.HTML(html)\n",
        "\n",
        "Out\\[6\\]:\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/dff484ba64d65de777e8449b84aface505c2ce31.jpg)\n",
        "\n",
        "In \\[ \\]:\n",
        "\n",
        "    # Importar PCA\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Crear una instancia de PCA con 2 componentes: pca\n",
        "    pca = PCA(n_components=2)\n",
        "\n",
        "    # Ajustar la instancia de PCA a las muestras escaladas\n",
        "    pca.fit(X_resampled)\n",
        "\n",
        "In \\[53\\]:\n",
        "\n",
        "    # Importar PCA\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Crear una instancia de PCA con 2 componentes: pca\n",
        "    pca = PCA(n_components=2)\n",
        "\n",
        "    pca_features = pca.fit_transform(X_resampled)\n",
        "\n",
        "    # Asignar la columna 0 de pca_features: xs\n",
        "    xs = pca_features[:,0]\n",
        "\n",
        "    # Asignar la primera columna de pca_features: ys\n",
        "    ys = pca_features[:,1]\n",
        "\n",
        "In \\[61\\]:\n",
        "\n",
        "    plt.scatter(xs, ys)\n",
        "    plt.axis()\n",
        "    plt.show()\n",
        "\n",
        "![](attachment:vertopal_85f4442851b444a08fa4b7c091ee224f/5e683f2174c891335faae8f67e41b15248953107.png)\n",
        "\n",
        "<span class=\"image\">ortogonales</span>\n",
        "\n",
        "### En función de las formas de los límites de decisión anteriores, pensamos que el árbol de decisión o el *random forest* serían una buena opción. <a href=\"#Based-on-decision-boundary-shapes-above,-we-think-either-decision-tree-or-random-forest-decision-boundaries-would-do-a-good-work\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "### Sin embargo, podríamos estar condicionados, ya que los resultados de precisión/recuperación y de la matriz de confusión también mostraron una solidez similar a la de *random forest*. <a href=\"#We-may-be-biased-though-since-precision/recall-and-confusion-matrix-results-also-showed-the-strength-of-Random-Forest\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[79\\]:\n",
        "\n",
        "    # Contar el número total de datos observacionales a partir de la longitud de y\n",
        "    total_obs = len(y)\n",
        "\n",
        "    # Contar el número total de datos observacionales no fraudulentos \n",
        "    non_fraud = [i for i in y if i == 0]\n",
        "    count_non_fraud = non_fraud.count(0)\n",
        "\n",
        "    # Calcular el porcentaje de datos no fraudulentos en el conjunto de datos\n",
        "    percentage = (float(count_non_fraud)/float(total_obs)) * 100\n",
        "\n",
        "    # Mostrar en pantalla el porcentaje: esta es la \"exactitud natural\" si no se hace nada\n",
        "    print(percentage)\n",
        "\n",
        "    99,777\n",
        "\n",
        "Con *random forest* se pueden crear subconjuntos aleatorios de las\n",
        "variables y construir árboles más pequeños utilizando dichos\n",
        "subconjuntos. Posteriormente, el algoritmo combina los subárboles de\n",
        "submuestras de las variables y promedia los resultados.\n",
        "\n",
        "In \\[82\\]:\n",
        "\n",
        "    # Importar los paquetes para obtener las diferentes mediciones de rendimiento\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    # Obtener las predicciones del modelo de random forest\n",
        "    predicted = model.predict(X_test)\n",
        "\n",
        "    # Predecir probabilidades\n",
        "    probs = model.predict_proba(X_test)\n",
        "\n",
        "    # Mostrar en pantalla la curva ROC, el informe de clasificación y la matriz de confusión\n",
        "    print(roc_auc_score(y_test, probs[:,1]))\n",
        "    #print(classification_report(y_test, predicted))\n",
        "    print(confusion_matrix(y_test, predicted))\n",
        "\n",
        "    /Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
        "      \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
        "\n",
        "    0,9829548960413107\n",
        "    [[29938     3]\n",
        "     [    3    56]]\n",
        "\n",
        "### Monitorizamos que el algoritmo *random forest* proporcionó menos exactitud que la estimación natural mediante el uso de una medición de precisión del rendimiento general. Sin embargo, nos parece más fiable y funciona considerablemente mejor que un enfoque tradicional basado en reglas. <a href=\"#We-monitor-Random-Forest-algorithm-provided-less-accuracy-than-natural-guess-by-using-a-general-performance-accuracy-metric.-However,-it-seems-more-reliable-to-us-and-performs-significantly-better-than-a-traditional-rule-based-approach\" class=\"anchor-link\">¶</a>\n",
        "\n",
        "In \\[ \\]:"
      ],
      "id": "d8jlMKFRAW4C"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Proyecto Final - Univ. Chicago - Random Forest detección de fraudes tarjetas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  }
}